# iam
Dataphion Assignment


In this project, we aim to develop a Spark job that consumes events from Apache Kafka and inserts the data into Iceberg, facilitating efficient storage and querying of event data. The tables will consist of columns such as Event Name, Event Type, Event Value, Event Timestamp, Event Page Source, Event Page URL, Event Component ID, User ID, and Event Date. Once the ingestion process is complete, the Spark job will generate daily, weekly, and monthly reports, offering insights into the top 10 events and their counts for a given date, the number of returning users, active users, and churn rates. By leveraging Spark's processing capabilities and integrating with Kafka for real-time data consumption, this solution ensures timely analysis and reporting of event data, enabling stakeholders to make informed decisions based on actionable insights.
